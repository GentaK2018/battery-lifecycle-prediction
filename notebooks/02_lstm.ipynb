{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0675843-5e9a-4a18-93f7-b02fd1634008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6950e47-6864-4b76-a71f-931a0cf4eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from copy import deepcopy\n",
    "import configparser\n",
    "\n",
    "import models\n",
    "import my_eval\n",
    "import severson_data\n",
    "from loss_functions import nll_loss\n",
    "import random\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b396e5-63f3-40a7-b152-b8ba29318e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(\n",
    "    model,\n",
    "    idxs,\n",
    "    x,\n",
    "    y,\n",
    "    augmented_data,\n",
    "    seq_length,\n",
    "    device,\n",
    "    scaler,\n",
    "    max_steps=5000,\n",
    "    use_cycle_counter=True,\n",
    "):\n",
    "    \"\"\"LSTMモデルから予測したサイクル寿命についてRMSEを計算する関数\n",
    "    \"\"\"\n",
    "\n",
    "    if use_cycle_counter:\n",
    "\n",
    "        supp_val_data = np.hstack(\n",
    "            [augmented_data[idxs], np.ones((len(idxs), 1)) * np.log(seq_length)]\n",
    "        )\n",
    "    else:\n",
    "        supp_val_data = augmented_data[idxs]\n",
    "\n",
    "    # seq_length=100とすると最初の100サイクル目までのデータを初期データとする。\n",
    "    test_seq = x[idxs][:, :seq_length, None].copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 全てのidxsについて、放電容量の予測値がほぼ0になるまで繰り返す。\n",
    "        while (np.all(test_seq[:, -1] < 1e-3) == False) * (\n",
    "            test_seq.shape[1] < max_steps\n",
    "        ):\n",
    "            # for i in range(max_steps - seq_length):\n",
    "            supp_val_data_torch = torch.from_numpy(supp_val_data).to(device).float()\n",
    "            test_seq_torch = (\n",
    "                torch.from_numpy(test_seq[:, -seq_length:]).to(device).float()\n",
    "            )\n",
    "            model.reset_hidden_state()\n",
    "            (pred_state, _) = model(test_seq_torch, supp_val_data_torch)\n",
    "\n",
    "            pred_state = torch.from_numpy(\n",
    "                scaler.inverse_transform(pred_state.cpu().numpy())\n",
    "            ).to(device)\n",
    "\n",
    "            # ((x[i, j + seq_len - 5: j + seq_len ]).mean() + 10e-17)\n",
    "            pred_state = pred_state[:, 0] * (test_seq_torch[:, -1, 0] + 10e-17)\n",
    "            # 配列の末尾に予測値をつなげる。\n",
    "            test_seq = np.hstack([test_seq, pred_state.cpu().numpy()[:, None, None]])\n",
    "            if use_cycle_counter:\n",
    "                supp_val_data[:, -1] = np.log(np.exp(supp_val_data[:, -1]) + 1)\n",
    "        # 0になったインデックスをサイクル寿命の予測値とする。\n",
    "        y_lifetime_pred = (test_seq[:, :, 0] < cutoff_val).argmax(axis=1)\n",
    "\n",
    "    return np.sqrt((np.square(y[idxs] - y_lifetime_pred)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d441c8a8-e692-4d59-8255-53a9e851ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"train.npz\")\n",
    "valid_data = np.load(\"valid.npz\")\n",
    "data = np.load(\"data.npz\")\n",
    "\n",
    "train_x, train_y, train_s = train_data[\"x\"], train_data[\"y\"], train_data[\"s\"]\n",
    "val_x, val_y, val_s = valid_data[\"x\"], valid_data[\"y\"], valid_data[\"s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9468aa0-c114-4965-8a08-9e3fb9413d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_x = data[\"old_x\"]\n",
    "smoothed_x = data[\"smoothed_x\"]\n",
    "augmented_data = data[\"augmented_data\"]\n",
    "y = data[\"y\"]\n",
    "train_idx = data[\"train_idx\"]\n",
    "val_idx = data[\"val_idx\"]\n",
    "test_idx = data[\"test_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bcc87ed-4a04-4fe4-ab86-a3ddb98ac9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"../sample_config.ini\")\n",
    "\n",
    "save_path = config[\"PATHS\"][\"model_path_severson\"]\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bab4bdc1-20e0-4c91-8767-8f49a6405e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "sequence_length = 100\n",
    "dropout = 0\n",
    "hidden_size_lstm = -1\n",
    "hidden_size = 32\n",
    "use_augment = 1\n",
    "no_covariates = False\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10146de3-fbfa-43f6-a4d5-7de337458da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "min_val = 0.85\n",
    "max_val = 1.0\n",
    "capacity_output_scaler = MinMaxScaler(\n",
    "    (-1, 1),\n",
    "    clip=False).fit(np.maximum(np.minimum(train_y[:, 0:1], max_val), min_val))\n",
    "\n",
    "train_y[:, 0:1] = capacity_output_scaler.transform(train_y[:, 0:1])\n",
    "val_y[:, 0:1] = capacity_output_scaler.transform(val_y[:, 0:1])\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "train_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [train_x, train_y, train_s]])\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    *[torch.Tensor(input) for input in [val_x, val_y, val_s]])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e20fb72-15fe-475e-b8b9-203cac2ea7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalerの保存\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pkl.dump(capacity_output_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5542af-0e67-484d-96dc-dc2ecc92c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 300\n",
    "sequence_length = 100\n",
    "dropout = 0\n",
    "hidden_size_lstm = -1\n",
    "hidden_size = 32\n",
    "use_augment = 1\n",
    "no_covariates = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e117a37-d4aa-4d95-abd0-3ba853b78d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: -0.79824, Validation loss: -0.97124, \n",
      "Epoch: 2, Training loss: -1.04365, Validation loss: -1.10705, \n",
      "Epoch: 3, Training loss: -1.63365, Validation loss: -2.82952, \n",
      "Epoch: 4, Training loss: -2.95263, Validation loss: -3.18665, \n",
      "Epoch: 5, Training loss: -3.28037, Validation loss: -3.54627, \n",
      "Epoch: 6, Training loss: -3.40294, Validation loss: -3.67330, \n",
      "Epoch: 7, Training loss: -3.46708, Validation loss: -3.56581, \n",
      "Epoch: 8, Training loss: -3.50308, Validation loss: -3.38105, \n",
      "Epoch: 9, Training loss: -3.55959, Validation loss: -3.63229, \n",
      "Epoch: 10, Training loss: -3.59713, Validation loss: -3.71536, \n",
      "Epoch: 11, Training loss: -3.61238, Validation loss: -3.70894, \n",
      "Epoch: 12, Training loss: -3.64877, Validation loss: -3.35464, \n",
      "Epoch: 13, Training loss: -3.65959, Validation loss: -3.62018, \n",
      "Epoch: 14, Training loss: -3.68335, Validation loss: -3.16207, \n",
      "Epoch: 15, Training loss: -3.67777, Validation loss: -3.69555, \n",
      "Epoch: 16, Training loss: -3.71316, Validation loss: -3.64427, \n",
      "Saved\n",
      "Epoch: 1, Training loss: -0.43191, Validation loss: -0.92276, \n",
      "Epoch: 2, Training loss: -0.97759, Validation loss: -0.94361, \n",
      "Epoch: 3, Training loss: -1.01977, Validation loss: -0.96296, \n",
      "Epoch: 4, Training loss: -1.07507, Validation loss: -1.01915, \n",
      "Epoch: 5, Training loss: -1.56746, Validation loss: -2.55339, \n",
      "Epoch: 6, Training loss: -2.91021, Validation loss: -3.28778, \n",
      "Epoch: 7, Training loss: -3.21872, Validation loss: -3.18352, \n",
      "Epoch: 8, Training loss: -3.37947, Validation loss: -3.66619, \n",
      "Epoch: 9, Training loss: -3.47808, Validation loss: -3.82152, \n",
      "Epoch: 10, Training loss: -3.51461, Validation loss: -3.86718, \n",
      "Epoch: 11, Training loss: -3.57448, Validation loss: -3.86179, \n",
      "Epoch: 12, Training loss: -3.61844, Validation loss: -3.87392, \n",
      "Epoch: 13, Training loss: -3.65463, Validation loss: -3.87462, \n",
      "Epoch: 14, Training loss: -3.69253, Validation loss: -3.87788, \n",
      "Epoch: 15, Training loss: -3.72362, Validation loss: -3.79929, \n",
      "Epoch: 16, Training loss: -3.75034, Validation loss: -3.88255, \n",
      "Epoch: 17, Training loss: -3.76702, Validation loss: -2.69992, \n",
      "Epoch: 18, Training loss: -3.74134, Validation loss: -3.86750, \n",
      "Epoch: 19, Training loss: -3.77414, Validation loss: -3.60379, \n",
      "Epoch: 20, Training loss: -3.80027, Validation loss: -3.81004, \n",
      "Epoch: 21, Training loss: -3.80142, Validation loss: -3.84511, \n",
      "Epoch: 22, Training loss: -3.81832, Validation loss: -3.84694, \n",
      "Saved\n",
      "Epoch: 1, Training loss: -0.66636, Validation loss: -0.91277, \n",
      "Epoch: 2, Training loss: -0.97793, Validation loss: -0.93428, \n",
      "Epoch: 3, Training loss: -1.03710, Validation loss: -1.03707, \n",
      "Epoch: 4, Training loss: -1.67184, Validation loss: -2.78733, \n",
      "Epoch: 5, Training loss: -2.93150, Validation loss: -3.26400, \n",
      "Epoch: 6, Training loss: -3.20871, Validation loss: -3.43698, \n",
      "Epoch: 7, Training loss: -3.32961, Validation loss: -3.48793, \n",
      "Epoch: 8, Training loss: -3.46120, Validation loss: -3.66218, \n",
      "Epoch: 9, Training loss: -3.49772, Validation loss: -3.73994, \n",
      "Epoch: 10, Training loss: -3.54991, Validation loss: -3.43790, \n",
      "Epoch: 11, Training loss: -3.57847, Validation loss: -3.85208, \n",
      "Epoch: 12, Training loss: -3.59527, Validation loss: -3.87119, \n",
      "Epoch: 13, Training loss: -3.64818, Validation loss: -3.91056, \n",
      "Epoch: 14, Training loss: -3.64588, Validation loss: -3.87633, \n",
      "Epoch: 15, Training loss: -3.67454, Validation loss: -3.89468, \n",
      "Epoch: 16, Training loss: -3.70183, Validation loss: -3.90265, \n",
      "Epoch: 17, Training loss: -3.70551, Validation loss: -3.80662, \n",
      "Epoch: 18, Training loss: -3.73375, Validation loss: -3.84799, \n",
      "Epoch: 19, Training loss: -3.76267, Validation loss: -3.39236, \n",
      "Saved\n",
      "Epoch: 1, Training loss: -0.57944, Validation loss: -0.91107, \n",
      "Epoch: 2, Training loss: -0.97740, Validation loss: -0.93792, \n",
      "Epoch: 3, Training loss: -1.01999, Validation loss: -0.95758, \n",
      "Epoch: 4, Training loss: -1.05356, Validation loss: -0.95224, \n",
      "Epoch: 5, Training loss: -1.12305, Validation loss: -1.09670, \n",
      "Epoch: 6, Training loss: -1.55827, Validation loss: -2.12970, \n",
      "Epoch: 7, Training loss: -2.77374, Validation loss: -3.07372, \n",
      "Epoch: 8, Training loss: -3.13804, Validation loss: -3.33222, \n",
      "Epoch: 9, Training loss: -3.32898, Validation loss: -3.32577, \n",
      "Epoch: 10, Training loss: -3.41769, Validation loss: -3.62867, \n",
      "Epoch: 11, Training loss: -3.49040, Validation loss: -3.73453, \n",
      "Epoch: 12, Training loss: -3.55403, Validation loss: -3.76979, \n",
      "Epoch: 13, Training loss: -3.58628, Validation loss: -3.80643, \n",
      "Epoch: 14, Training loss: -3.61643, Validation loss: -3.78015, \n",
      "Epoch: 15, Training loss: -3.64097, Validation loss: -3.78968, \n",
      "Epoch: 16, Training loss: -3.62453, Validation loss: -3.79050, \n",
      "Epoch: 17, Training loss: -3.67444, Validation loss: -3.79277, \n",
      "Epoch: 18, Training loss: -3.67921, Validation loss: -3.73239, \n",
      "Epoch: 19, Training loss: -3.66664, Validation loss: -3.77326, \n",
      "Saved\n",
      "Epoch: 1, Training loss: -0.36673, Validation loss: -0.91685, \n",
      "Epoch: 2, Training loss: -0.99962, Validation loss: -0.94021, \n",
      "Epoch: 3, Training loss: -1.02799, Validation loss: -0.95404, \n",
      "Epoch: 4, Training loss: -1.05526, Validation loss: -0.99072, \n",
      "Epoch: 5, Training loss: -1.14350, Validation loss: -1.14412, \n",
      "Epoch: 6, Training loss: -1.91433, Validation loss: -2.92934, \n",
      "Epoch: 7, Training loss: -2.85797, Validation loss: -3.21723, \n",
      "Epoch: 8, Training loss: -3.11956, Validation loss: -3.54508, \n",
      "Epoch: 9, Training loss: -3.30769, Validation loss: -3.66256, \n",
      "Epoch: 10, Training loss: -3.45351, Validation loss: -3.25514, \n",
      "Epoch: 11, Training loss: -3.50899, Validation loss: -3.80568, \n",
      "Epoch: 12, Training loss: -3.59538, Validation loss: -3.88590, \n",
      "Epoch: 13, Training loss: -3.62435, Validation loss: -3.57740, \n",
      "Epoch: 14, Training loss: -3.66196, Validation loss: -3.67507, \n",
      "Epoch: 15, Training loss: -3.70816, Validation loss: -3.79983, \n",
      "Epoch: 16, Training loss: -3.75447, Validation loss: -3.69415, \n",
      "Epoch: 17, Training loss: -3.78382, Validation loss: -3.31838, \n",
      "Epoch: 18, Training loss: -3.77177, Validation loss: -3.72543, \n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "for im in range(5):  # アンサンブルのため5個のモデルを作る\n",
    "    seed = int(42*im)\n",
    "    \n",
    "    experiment_name = f\"model{im}\"\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    input_dim = train_x.shape[\n",
    "        2]  # Number of input features (e.g. discharge capacity)\n",
    "    num_augment = train_s.shape[1]\n",
    "\n",
    "    if no_covariates:\n",
    "        model = models.Uncertain_LSTM_NoCovariate(\n",
    "            num_in=input_dim,\n",
    "            num_augment=num_augment,\n",
    "            num_hidden=hidden_size,\n",
    "            num_hidden_lstm=hidden_size_lstm,\n",
    "            seq_len=sequence_length,\n",
    "            n_layers=2,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = models.Uncertain_LSTM(\n",
    "            num_in=input_dim,\n",
    "            num_augment=num_augment,\n",
    "            num_hidden=hidden_size,\n",
    "            num_hidden_lstm=hidden_size_lstm,\n",
    "            seq_len=sequence_length,\n",
    "            n_layers=2,\n",
    "        ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), )\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "\n",
    "    best_val_loss = 500000\n",
    "\n",
    "    cur_patience = 0\n",
    "    max_patience = 5\n",
    "    patience_delta = 0.0\n",
    "    best_weights = None\n",
    "    \n",
    "    # 学習\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "\n",
    "        for batch_idx, (\n",
    "                input_data,\n",
    "                y_hat,\n",
    "                supp_data,\n",
    "        ) in enumerate(train_loader):\n",
    "            model.reset_hidden_state()\n",
    "            input_data = input_data.to(device)\n",
    "            supp_data = supp_data.to(device)\n",
    "            y_hat = y_hat.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            (state_mean, state_var) = model(input_data, supp_data)\n",
    "\n",
    "            # loss\n",
    "            loss_state = nll_loss(y_hat[:, 0], state_mean[:, 0], state_var[:, 0])\n",
    "            loss = loss_state\n",
    "            (loss).backward()\n",
    "            tr_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        tr_loss /= len(train_loader.dataset)\n",
    "        training_loss.append(tr_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_loss_state = 0\n",
    "        val_loss_lifetime = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (\n",
    "                    input_data,\n",
    "                    y_hat,\n",
    "                    supp_data,\n",
    "            ) in enumerate(val_loader):\n",
    "                model.reset_hidden_state()\n",
    "                input_data = input_data.to(device)\n",
    "                supp_data = supp_data.to(device)\n",
    "                y_hat = y_hat.to(device)\n",
    "\n",
    "                (state_mean, state_var) = model(input_data, supp_data)\n",
    "\n",
    "                loss_state = nll_loss(y_hat[:, 0], state_mean[:, 0], state_var[:,\n",
    "                                                                               0])\n",
    "                loss = loss_state\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_loss_state += loss_state.item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_loss_state /= len(val_loader.dataset)\n",
    "\n",
    "        val_loss_lifetime /= len(val_loader.dataset)\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        print(\"Epoch: %d, Training loss: %1.5f, Validation loss: %1.5f, \" % (\n",
    "            epoch + 1,\n",
    "            tr_loss,\n",
    "            val_loss,\n",
    "        ))\n",
    "\n",
    "        if val_loss + patience_delta < best_val_loss:\n",
    "            best_weights = deepcopy(model.state_dict())\n",
    "            cur_patience = 0\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            cur_patience += 1\n",
    "        if cur_patience > max_patience:\n",
    "            break\n",
    "\n",
    "\n",
    "    # 結果の保存\n",
    "    # np.random.seed()\n",
    "    # file_name = \"\".join([str(np.random.choice(10)) for x in range(10)])\n",
    "    file_name = experiment_name\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results[\"train_losses\"] = training_loss\n",
    "    results[\"val_losses\"] = validation_loss\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    model.eval()\n",
    "\n",
    "    cutoff_val = 10e-2\n",
    "\n",
    "    results[\"rmse_state_train\"] = get_rmse(\n",
    "        model=model,\n",
    "        idxs=train_idx,\n",
    "        x=old_x,\n",
    "        y=y,\n",
    "        augmented_data=augmented_data,\n",
    "        seq_length=100,\n",
    "        device=device,\n",
    "        scaler=capacity_output_scaler,\n",
    "    )\n",
    "\n",
    "    results[\"rmse_state_val\"] = get_rmse(\n",
    "        model=model,\n",
    "        idxs=val_idx,\n",
    "        x=old_x,\n",
    "        y=y,\n",
    "        augmented_data=augmented_data,\n",
    "        seq_length=100,\n",
    "        device=device,\n",
    "        scaler=capacity_output_scaler,\n",
    "    )\n",
    "\n",
    "    results[\"rmse_state_test\"] = get_rmse(\n",
    "        model=model,\n",
    "        idxs=test_idx,\n",
    "        x=old_x,\n",
    "        y=y,\n",
    "        augmented_data=augmented_data,\n",
    "        seq_length=100,\n",
    "        device=device,\n",
    "        scaler=capacity_output_scaler,\n",
    "    )\n",
    "\n",
    "    results[\"file_name\"] = file_name\n",
    "    results[\"best_val_loss\"] = best_val_loss\n",
    "\n",
    "    pkl.dump(results, open(os.path.join(save_path, file_name + \".pkl\"), \"wb\"))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, file_name + \".pt\"))\n",
    "    print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af6bbdbc-f0f7-4034-b033-ecdd7a01ab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 7])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80763941-a847-48f6-8e05-e70f5d1772e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"{save_path}/model_test.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
